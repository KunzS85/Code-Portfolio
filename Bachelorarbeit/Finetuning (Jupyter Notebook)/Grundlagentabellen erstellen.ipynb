{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Grundlagentabellen erstellen.ipynb","provenance":[],"authorship_tag":"ABX9TyMsJHHh2SJ47yKQa7bnxyDf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"I0jg8WjDdySV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install numpy \n","!pip install pandas\n","!pip install json"],"metadata":{"id":"Hk_cX2Mpd8-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Import für Datenmanipulation\n","import numpy as np #Für Matrizen\n","import pandas as pd #Für Dataframes\n","import re\n","from typing import List, Any, Set, Dict, Tuple, Optional\n","import json # load json module"],"metadata":{"id":"EZLG4BZld88g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TEXT_COLUMN = 'Text'\n","SNOMED_CT_CODE_COLUMN = 'Code'\n","LABEL_COLUMN = 'Label'\n","LABEL_CODE_COLUMN = 'Label Code'\n","OHE_COLUMN = 'One Hot Encoding'"],"metadata":{"id":"ClxyLgm_d86M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","  **Daten aus Snomed CT release Package generieren**\n","  \n","\n"],"metadata":{"id":"fRGyOTfPAWMZ"}},{"cell_type":"markdown","source":["Tabelle aus SNOMED CT Releasepackage erstellen"],"metadata":{"id":"nDNgH_QOCBDK"}},{"cell_type":"code","source":["# Install medcat (Quelle: https://colab.research.google.com/github/CogStack/MedCATtutorials/blob/main/notebooks/specialised/Preprocessing_SNOMED_CT.ipynb#scrollTo=CqUmB-ZH13zB)\n","!pip install --upgrade medcat\n","from medcat.utils.preprocess_snomed import Snomed\n","\n","# Load local zip folder into colab repository\n","from google.colab import files\n","uploaded = files.upload()\n","\n","snomed_path = 'SnomedCT_InternationalRF2_PRODUCTION_20220228T120000Z.zip' \n","!unzip snomed_path\n","\n","# Initialise\n","snomed_filename = \"SnomedCT_InternationalRF2_PRODUCTION_20210131T120000Z\"  # The unzippedSNOMED CT folder\n","snomed = Snomed(snomed_filename)\n","\n","df = snomed.to_concept()\n","\n","# Structure df is cui [0], name [1], name_status [2], ontologies [3] description_type_ids [4], type_ids [5]"],"metadata":{"id":"JF3HGM1Vd82Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SNOMED CT Tabelle einlesen, Typs: procedure, body structure (SNOMED CT) und disorder (NOT SNOMED CT) aus Tabelle extrahieren. Neues xlsx erstellen"],"metadata":{"id":"AgMm9Rj5CKkD"}},{"cell_type":"code","source":["import csv\n","import pandas as pd\n","from openpyxl import Workbook\n","\n","df = pd.read_csv(r'/content/drive/MyDrive/preprocessed_snomed.csv' , sep = \",\")  # read the csv file\n","\n","# Structure df is cui [0], name [1], name_status [2], ontologies [3] description_type_ids [4], type_ids [5]\n","text = []   \n","codeIDs = []\n","label = []\n","\n","for x in range(len(df)):\n","  y = df['type_ids'].values[x]\n","  if y == 28321150 or y == 37552161:\n","\n","    text.append(df['name'].values[x])\n","    codeIDs.append(df['cui'].values[x])\n","    label.append(\"SNOMED CT\")\n","\n","  elif y == 9090192:\n","    text.append(df['name'].values[x])\n","    codeIDs.append(df['cui'].values[x])\n","    label.append(\"NOT SNOMED CT\")\n","\n","new_df = pd.DataFrame({'Text': text,'Code': codeIDs, 'Label': label})\n","new_df.to_csv(r'/content/drive/MyDrive/snomed_train_df.csv', index=False)\n","\n","wb = Workbook()\n","ws = wb.active\n","ws.title = 'snomed_train_df'\n","with open('/content/drive/MyDrive/snomed_train_df.csv', 'r', encoding='utf8') as f:\n","    for row in csv.reader(f):\n","      ws.append(row)\n","    wb.save('content/drive/MyDrive/snomed_train_df.xlsx')\n"],"metadata":{"id":"6Qe-F5mPCORA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Master SNOMED CT Tabelle erstellen (Tabelle für einmalige Konzepte mit entsprechenden Synonymen. Grundlage ist die Tabelle aus dem SNOMED CT Releasepackage)**"],"metadata":{"id":"wVGrjUahCs9N"}},{"cell_type":"code","source":["code = {}\n","synonym = []\n","for index, row in df.iterrows():\n","  if row['cui'] in code:\n","    code[row['cui']][\"synonym\"].append(row['name'])\n","  elif row['description_type_ids'] == \"procedure\" or row['description_type_ids'] == \"body structure\":\n","    code[row['cui']] = {\n","        \"name\": row['name'],\n","        \"synonym\": []\n","        }\n","\n"],"metadata":{"id":"u-sIqm97P6_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(code))\n","c = []\n","n = []\n","s = []\n","for k, v in code.items():\n","  c.append(k)\n","  n.append(v[\"name\"])\n","  s.append(v[\"synonym\"])\n","\n","cd = { \n","    'Code': c,\n","    'name': n,\n","    'synonyms': s  \n","}\n","df = pd.DataFrame(cd)\n","print(df[\"Code\"])"],"metadata":{"id":"e4dYXSLQSMea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bereinigen des Hauptkonzeptnames "],"metadata":{"id":"e8X3lhlDgifb"}},{"cell_type":"code","source":["for index, row in df.iterrows(): \n"," text_without_proc = row['Text'].replace(\"(procedure)\", \"\")\n"," text_without_body_strac = text_without_proc.replace(\"(body structure)\", \"\")\n"," text_without_disorder = text_without_body_strac.replace(\"(disorder)\", \"\")\n"," row['Text'] = text_without_disorder\n","\n","print(df)"],"metadata":{"id":"3OvNcypNGVop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["excel_path = '/content/drive/MyDrive/master_data.xlsx'\n","sheet_name = \"snomed_master\"\n","print(df)\n","with pd.ExcelWriter(excel_path) as writer:  \n","    df.to_excel(writer, sheet_name=sheet_name, index=False)"],"metadata":{"id":"xXgCxP15Dl5b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Daten aus den zusammengeführten Weiterbildungstabellen SIWF (bereits parametriert) generieren**\n","  "],"metadata":{"id":"yr_iauKHAfup"}},{"cell_type":"markdown","source":["SIWF Tabellen einlesen, Quries auseinander nehmen und neues \"erweitertes\" Dataframe erstellen (Beschreibung, Code, Label)"],"metadata":{"id":"N8FJBDmSCXSM"}},{"cell_type":"code","source":["excel_path = '/content/drive/MyDrive/dataset.xlsx'\n","sheet_name = \"siwf_cleaned_translated\"\n","\n","df = pd.read_excel(excel_path, sheet_name)\n","#print(df)\n","\n","codeIDs = []\n","for x in df['Code']:\n","  #print(x)\n","  if (type(x) is str):\n","    id_lst = re.findall(r'\\d+', x)\n","    #print(id_lst)\n","    codeIDs.append(id_lst)\n","  else:\n","    codeIDs.append([])\n","#print(codeIDs)\n","df['CodeIDs'] = codeIDs\n","\n","text = []\n","code = []\n","label = []\n","for i in range(len(df)):\n","    for c in df.iloc[i,5]:\n","      text.append(df.iloc[i,1])\n","      code.append(c)\n","      label.append(df.iloc[i,0])\n","      \n","siwf_train_df = pd.DataFrame({TEXT_COLUMN : text,\n","                         SNOMED_CT_CODE_COLUMN : code,\n","                         LABEL_COLUMN : label\n","                         })\n","    \n","print(siwf_train_df)"],"metadata":{"id":"VVo-yl5vBXuy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["excel_path = '/content/drive/MyDrive/train_data.xlsx'\n","siwf_sheet_name = \"siwf_train_df\"\n","print(siwf_train_df)\n","with pd.ExcelWriter(excel_path) as writer:  \n","    siwf_train_df.to_excel(writer, sheet_name=siwf_sheet_name, index=False)"],"metadata":{"id":"QlhevSsMCdy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Maximale Länge der tokenisierten Texte überprüfen** \n","(Nach dem Anreichern von Texten mit Kontext kann so überprüft werden, wie lange die Sätze sind. => Problem ist, dass BERT beim tokenisieren ein eigenes Stemming macht und entsprechend Wörter mehrere Tokens sein können) Als Output der Methode wird die länge des längsten Satzes im DF ausgegeben. "],"metadata":{"id":"HNDqBqYuqZ01"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n","\n","max_len = 0\n","\n","sentences = df.Text.values\n","\n","# For every sentence...\n","for sent in sentences:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"id":"5JsQamppqkft"},"execution_count":null,"outputs":[]}]}